# Profile Analysis Instructions

## How to Analyze samply Profile JSON

The profile JSON generated by `samply` uses the Firefox Profiler format. Here's how to extract hot spots:

Look for the thread with the name `module_benchmarks`

### Python Script to Extract Top Functions

```python
import json
from collections import defaultdict

# Load the profile
with open('path/to/profile.json', 'r') as f:
    data = json.load(f)

# Get the shared string array
strings = data['shared']['stringArray']

# Analyze each thread
for thread_idx, thread in enumerate(data['threads']):
    if 'samples' not in thread:
        continue

    samples = thread['samples']
    stack_table = thread.get('stackTable', {})
    frame_table = thread.get('frameTable', {})
    func_table = thread.get('funcTable', {})

    # Get function names for this thread
    func_names = func_table.get('name', [])

    # Get sample stacks (each sample points to a stack frame)
    sample_stacks = samples.get('stack', [])

    # Count samples per function (self-time = top of stack)
    self_counts = defaultdict(int)

    for stack_idx in sample_stacks:
        if stack_idx is not None and stack_idx < len(stack_table.get('frame', [])):
            # Get the frame at the top of this stack
            frame_idx = stack_table['frame'][stack_idx]
            if frame_idx < len(frame_table.get('func', [])):
                func_idx = frame_table['func'][frame_idx]
                if func_idx < len(func_names):
                    name_idx = func_names[func_idx]
                    if name_idx < len(strings):
                        func_name = strings[name_idx]
                        self_counts[func_name] += 1

    # Print results
    print(f'\n=== Thread {thread_idx}: {thread.get("name", "unknown")} ===')
    print(f'Total samples: {len(sample_stacks)}')
    print('\nTop 30 functions by self-time:')

    sorted_funcs = sorted(self_counts.items(), key=lambda x: -x[1])
    for i, (func, count) in enumerate(sorted_funcs[:30], 1):
        pct = (count / len(sample_stacks) * 100) if sample_stacks else 0
        print(f'{i:2}. {count:>5} ({pct:5.1f}%): {func[:90]}')
```

### Usage

1. **Generate profile:**
   ```bash
   samply record cargo bench --bench parser_benchmarks
   ```

2. **Run analysis:**
   ```bash
   python3 analyze_profile.py
   ```

3. **Look for the benchmark thread:**
   - Usually named something like `parser_benchmarks-<hash>` or similar
   - Has the most samples (thousands vs dozens for other threads)

### Understanding the Output

- **Self-time %**: Time spent IN that function (not including callees)
- **Sample count**: Number of times profiler caught function at top of stack
- Higher percentage = bigger bottleneck

### Key Metrics from Latest Profile

From the profile `cargo 2025-11-24 15.20 profile.json`, Thread 15 shows:

**Total samples: 12,387**

**Top hot spots:**
1. `Parser::fill_buffer` - 20.6% (2556 samples)
2. `Lexer::scan_token` - 13.4% (1665 samples)
3. `Cursor::eat_while` - 11.0% (1363 samples)
4. `Cursor::advance` - 10.1% (1247 samples)
5. `_platform_memmove` - 7.0% (869 samples)
6. `Lexer::scan_operator` - 4.3% (536 samples)

**Lexer total: 46.8%** (scan_token + eat_while + advance + scan_operator)

### Alternative: Firefox Profiler UI

You can also upload the JSON to https://profiler.firefox.com for visual analysis:

1. Open https://profiler.firefox.com
2. Load your profile JSON file
3. Look at the "Flame Graph" tab
4. Check "Call Tree" for self-time percentages
5. Filter to your benchmark thread

### What to Look For

**In Rust code:**
- Functions with `angelscript::` prefix
- Parser, Lexer, and Cursor methods
- High percentage self-time (>5%)

**System functions indicating problems:**
- `_platform_memmove` / `memcpy` → Vec reallocations
- `malloc` / `free` → Too many small allocations
- `__psynch_cvwait` → Thread synchronization overhead

### Current Status

**Baseline:** 2.28ms (before arena allocation)
**After arena:** 1.31ms (73-75% improvement)
**Target:** <1ms (need ~24% more improvement)

The profiling shows lexer is the main bottleneck at 46.8% of total time.
